{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.documents import Document\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = ParsedPDFLoader(\"parsed/wf.json\")\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Indexing : Store\n",
    "Creating embeddings for the splitted data and store the documents and it's corresponsing embeddings in a vector store. At this point we have a query-able vector store containing the chunked contents of our PDF's. Given a user question, we should ideally be able to return the snippets of the text that answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(documents=loader.get_documents(), embedding=OllamaEmbeddings())\n",
    "vectorstore.save_local('vectorstores/wf_vectorstore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.load_local(\"./vectorstore\",OllamaEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity Search\n",
    "query = \"Bank of America Chair & CEO?\"\n",
    "docs = vectorstore.similarity_search(query, k=3)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity Search\n",
    "query = \"Who is the CEO of Amazon?\"\n",
    "docs = vectorstore.similarity_search(query, k=3)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Retrieval and Generation: Retrieve\n",
    "Different ways to retrive documents based on query - [Link](https://python.langchain.com/docs/modules/data_connection/retrievers/vectorstore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Using the query as is to retrieve the relevant documents using the search type \"similarity_score_threshold\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_similarity = vectorstore.as_retriever(search_type=\"similarity_score_threshold\",\n",
    "                                                 search_kwargs={\"score_threshold\": 0.70})\n",
    "# retrieved_docs = retriever.invoke(\"How innovation is driven at bank of america?\")\n",
    "retrieved_docs = retriever_similarity.get_relevant_documents(\"Who is the CEO of Amazon\") #What is the full-form of ROTCE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in retrieved_docs:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Using the Multi Query retriver which will create variants of queries based on the prompt to retrieve the relevant documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supply a prompt along with an output parser to split the results into a list of queries.\n",
    "# Output parser will split the LLM result into a list of queries\n",
    "class LineList(BaseModel):\n",
    "    # \"lines\" is the key (attribute name) of the parsed output\n",
    "    lines: List[str] = Field(description=\"Lines of text\")\n",
    "\n",
    "\n",
    "class LineListOutputParser(PydanticOutputParser):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(pydantic_object=LineList)\n",
    "\n",
    "    def parse(self, text: str) -> LineList:\n",
    "        lines = text.strip().split(\"\\n\")\n",
    "        return LineList(lines=lines)\n",
    "\n",
    "\n",
    "output_parser = LineListOutputParser()\n",
    "\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate four \\ \n",
    "    different versions of the given user question to retrieve relevant documents from a vector \\\n",
    "    database. By generating multiple perspectives on the user question, your goal is to help \\\n",
    "    the user overcome some of the limitations of the distance-based similarity search. \\ \n",
    "    Provide these alternative questions separated by newlines. \\\n",
    "    Original question: {question}\"\"\",\n",
    ")\n",
    "llm = ChatOpenAI(temperature=0, model='gpt-3.5-turbo-16k')\n",
    "\n",
    "\n",
    "# Chain\n",
    "llm_chain = LLMChain(llm=llm, prompt=QUERY_PROMPT, output_parser=output_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "retriever = MultiQueryRetriever(\n",
    "    include_original=True,\n",
    "    retriever=vectorstore.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.70}),\n",
    "    llm_chain=llm_chain, parser_key=\"lines\",\n",
    "    verbose=True\n",
    ")  # \"lines\" is the key (attribute name) of the parsed output\n",
    "\n",
    "# Results\n",
    "unique_docs = retriever.get_relevant_documents(\n",
    "    query=\"What is tuition assistance?\"\n",
    ")\n",
    "len(unique_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check different queries generated by the llm\n",
    "llm_chain.invoke(\"What is tuition assistance?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Retrieval and Generation: Generate\n",
    "In this step everything will be put together into a chain. A chain will take question, will retrieve relevant documents, will construct a prompt, will pass to the llm model and will parse the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contextualize_ques_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "# which might refer to a context in the chat history, formulate a standalone question \\\n",
    "# which can be understood without the chat history. Do NOT answer the question, \\\n",
    "# just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "\n",
    "contextualize_ques_system_prompt = \"\"\" Check if the latest user question refers to a context in the chat history. \\\n",
    "If it does, formulate a standalone question which can be understood without the chat history. \\\n",
    "If it does not, return the question as is. Your task is to just reformulate the question if needed.\n",
    "\"\"\"\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_ques_system_prompt),\n",
    "        # Prompt template that assumes variable is already list of messages.\n",
    "        # We provide the variable name to be used as messages\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "contextualize_q_chain = contextualize_q_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just say that you don't know.\n",
    "# qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "# Use the following pieces of retrieved context to answer the question. \\\n",
    "# If you don't know the answer, provide a response as - \"I don't know.\" \\\n",
    "\n",
    "# {context}\"\"\"\n",
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following context or document embeddings to answer the question. \\\n",
    "Don't justify your answers and don't search the internet.If the answer could not be found in the documents,\\\n",
    "ay the words \"Sorry, I am unable to answer your question with the information available to me\"\\\n",
    "\n",
    "{context}\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def contextualized_question(input: dict):\n",
    "    if input.get(\"chat_history\"):        \n",
    "        return contextualize_q_chain\n",
    "    else:\n",
    "        return input[\"question\"]\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        context=contextualized_question | retriever_similarity\n",
    "    )\n",
    "    | qa_prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "\n",
    "question = \"How Risk Management is done in Wells Fargo?\"\n",
    "ai_msg = rag_chain.invoke({\"question\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=question), ai_msg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_msg.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_question = \"How Wells Fargo protect against unauthorized access?\"\n",
    "new_msg = rag_chain.invoke({\"question\": second_question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=second_question), new_msg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_msg.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_question = \"Who is the CEO of Amazon?\"\n",
    "new_msg = rag_chain.invoke({\"question\": third_question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=third_question), new_msg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_msg.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourth_question = \"What did Sheri Bronstein wrote in her letter in Bank of America document?\"\n",
    "new_msg = rag_chain.invoke({\"question\": fourth_question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=fourth_question), new_msg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_msg.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fifth_question = \"What is Emotional Wellness?\"\n",
    "new_msg = rag_chain.invoke({\"question\": fifth_question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=fifth_question), new_msg])\n",
    "new_msg.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_question = \"What are different types of Risk Management at Wells Fargo?\"\n",
    "rag_chain.invoke({\"question\": third_question, \"chat_history\": chat_history}).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear chat history\n",
    "chat_history.clear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
