{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easyocr\n",
    "import fitz  # PyMuPDF\n",
    "import glob\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from ollama import generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 4060 Ti\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())  # This should return True if CUDA is available and configured correctly\n",
    "\n",
    "print(torch.cuda.get_device_name(0))  # This prints the name of the CUDA device, confirming it's recognized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "# import os\n",
    "# import re\n",
    "# import dotenv\n",
    "# import fitz  # PyMuPDF\n",
    "# from collections import defaultdict\n",
    "# from langchain.document_loaders import UnstructuredPDFLoader\n",
    "# from unstructured.cleaners.core import clean_extra_whitespace\n",
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "# from langchain_community.vectorstores import FAISS\n",
    "# from langchain.chains import LLMChain\n",
    "# from langchain.output_parsers import PydanticOutputParser\n",
    "# from langchain.prompts import PromptTemplate\n",
    "# from pydantic import BaseModel, Field\n",
    "# from typing import List\n",
    "# from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "# # from langchain.chat_models import ChatOpenAI\n",
    "# from langchain_community.chat_models import ChatOpenAI\n",
    "# from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "# from langchain_core.runnables import RunnablePassthrough\n",
    "# from langchain_core.messages import HumanMessage\n",
    "# from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get tge api key from the .env file\n",
    "# dotenv.load_dotenv()\n",
    "# open_ai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "# print(open_ai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2022-annual-report-wf.pdf', '2022-annual-report-bofa.pdf']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a list of all .pdf files in the current directory\n",
    "pdf_files = glob.glob('*.pdf')\n",
    "\n",
    "pdf_documents = [file for file in pdf_files]\n",
    "pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_raw_images(pdf_file):\n",
    "    doc = fitz.open(pdf_file)  # Open the PDF file\n",
    "    raw_images = {}\n",
    "    \n",
    "\n",
    "    for page_num in range(len(doc)):  # Iterate through each page\n",
    "        page = doc.load_page(page_num)  # Load the current page\n",
    "        image_list = page.get_images(full=True)  # Get the list of images in the current page\n",
    "        \n",
    "        images = []\n",
    "        for img_info in image_list:\n",
    "            xref = img_info[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]  # The raw image bytes\n",
    "            images.append(image_bytes)\n",
    "        \n",
    "        if len(images) > 0:\n",
    "            raw_images[page_num] = images\n",
    "    \n",
    "    return raw_images\n",
    "\n",
    "def detect_text_in_image(raw_images):\n",
    "    reader = easyocr.Reader(['en'],gpu=True)  # Create the OCR reader\n",
    "    # Store the raw images where text was detected\n",
    "    text_detected_images = {}\n",
    "\n",
    "    for key, value in raw_images.items():\n",
    "        text_detected_raw_images = []\n",
    "\n",
    "        for image_bytes in value:\n",
    "            results = reader.readtext(image_bytes,detail=0)\n",
    "            if results:\n",
    "                # Check the length of results, because there are certain images with hashtag\n",
    "                # or no significant information\n",
    "                if len(results) > 10:\n",
    "                    text_detected_raw_images.append(image_bytes)\n",
    "        # Add only those raw images of the page which would be used \n",
    "        # in Llava to describe the image  \n",
    "        if len(text_detected_raw_images) > 0:            \n",
    "            text_detected_images[key] = text_detected_raw_images\n",
    "    \n",
    "    return text_detected_images\n",
    "\n",
    "prompt = \"\"\"Describe the contents in the image. Follow the below guidelines while generating the description of the image.\n",
    "1. Do not give the style or font details\n",
    "2. Do not describe image if it does not contain important information\n",
    "3. Do not start the description with \"The image\" or \"This image\". We know that it is an image,\n",
    "so there is no need to explicitly mention it.\n",
    "\"\"\"\n",
    "\n",
    "def extract_image_description(text_detected_images):\n",
    "    image_description = {}\n",
    "    \n",
    "\n",
    "    for key, value in text_detected_images.items():\n",
    "        page_image_description = []\n",
    "        \n",
    "        for raw_image in value:\n",
    "            response = generate(model = 'llava', prompt=prompt, images=[raw_image], stream=False)\n",
    "            \n",
    "            page_image_description.append(response['response'])\n",
    "\n",
    "        if len(page_image_description) > 0:            \n",
    "            image_description[key] = page_image_description\n",
    "\n",
    "    return image_description                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the description of the images to be used for the LLM for RAG\n",
    "raw_images = extract_raw_images(pdf_documents[1])\n",
    "text_detected_images = detect_text_in_image(raw_images)\n",
    "img_description_bofa = extract_image_description(text_detected_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{35: [' The image shows a smartphone displaying a mobile banking application. The screen is activated and displays an account summary for a customer named \"Robin.\" The account has an American Life Plan membership, as indicated by the logo of Bank of America. Robin\\'s account balance is $693.98, with a pending transaction of $102.75.\\n\\nThere is also a reward status at the top, stating that Robin has earned 1250 points for their transaction and has reached 6375 total points. A welcome message greets Robin with the text \"How can we help you today?\" The overall color scheme of the app is white and green, which is the Bank of America\\'s branding.\\n\\nBelow the account summary, there are additional details about a recent transaction. The transaction was made on \"MERRILL,\" and it was for $57.86, indicating that the transaction was not in USD but rather in another currency. There is also a pending charge of $102.75, which seems to be a different transaction from the one above.\\n\\nThe bottom navigation bar has four icons: Home, Activity, Rewards, and Bank. The screen also shows some of Robin\\'s personal information, such as their name, address, and phone number. However, these details are redacted for privacy. '],\n",
       " 85: [' The image shows a chart that seems to be an organizational chart for the Bank of America Corporation. At the top, there\\'s a section labeled \"Bank of America Global Markets,\" which includes subsidiaries and affiliates such as \"FedEx Global Trade,\" \"Global Capital Markets,\" and others.\\n\\nBelow this, another section is titled \"Bank of America Merchant Services,\" listing items like \"Merchants,\" \"Merchant Banking,\" and related services.\\n\\nThe chart continues with \"Bank of America Consumer & Small Business Banking,\" which includes \"Retail Banking,\" \"Digital Channels,\" and other consumer banking activities.\\n\\nFurther down, there\\'s a section for \"Bank of America Corporate & Investment Banking,\" which encompasses various corporate banking services.\\n\\nAt the bottom, the chart shows the \"Bank of America Retail Banking\" segment, which includes branches like \"Retail Banking,\" \"Digital Channels,\" and other retail banking activities.\\n\\nThe image also displays a list of subsidiaries or companies under \"Bank of America Corporation,\" including \"Wells Fargo,\" \"JP Morgan Chase & Co,\" \"Citigroup,\" and others, which suggests they are part of the Bank of America\\'s corporate structure. The chart is a visual representation of the Bank of America\\'s organizational structure, showing how its different departments and business units are structured. '],\n",
       " 95: [' The image displays an organizational chart with the title \"BOARD OF DIRECTORS\" at the top. The chart is structured in a hierarchical manner, with branches indicating different departments or areas of responsibility within a company.\\n\\nStarting from the top, there are two main branches: \"COMPLIANCE AND RISK MANAGEMENT\" and \"MANAGEMENT COMMITTEE.\" Under \"COMPLIANCE AND RISK MANAGEMENT,\" there is one sub-branch labeled \"COMPLIANCE COMMITTEE\" with a further branch named \"COMPLIANCE OFFICER/S.\"\\n\\nThe \"MANAGEMENT COMMITTEE\" branch has a single sub-branch titled \"MAINTENANCE OF EQUIPMENT.\"\\n\\nUnderneath the main title, there are additional branches representing different areas of management. The first one is \"FACILITIES,\" which has two sub-branches: \"FACILITIES MANAGER\" and \"CONSTRUCTION AND REMODELING.\"\\n\\nThe second area of management listed is \"MARKETING COMMITTEE,\" with a further branch named \"MARKETING OFFICER/S.\"\\n\\nThe third area is \"REGULATORY AFFAIRS COMMITTEE\" with a sub-branch titled \"COMPLIANCE OFFICER/S.\"\\n\\nThe last major area of management mentioned in the chart is \"HUMAN RESOURCES,\" which has one sub-branch labeled \"HUMAN RESOURCES MANAGER.\"\\n\\nEach branch and sub-branch is connected by lines, indicating their relationship to each other within the organizational structure. The text on the chart is in all capital letters. '],\n",
       " 125: [' The image displays a line graph with data points and a horizontal axis labeled \"Day to Goal (Days)\" ranging from 0 to over 200 days. There is a vertical axis on the right side of the image, which includes numerical values such as 0.1, 0.3, and so on. The graph line shows a general upward trend, with occasional fluctuations and dips in value.\\n\\nOn top of the graph, there is text that reads \"Daily Goal Probabilities and Logistic Regression of Goal Probabilities.\" Additionally, the image contains some mathematical or statistical formulas at the bottom right corner. The overall style of the image suggests it is a screenshot from a software interface used for data analysis or modeling purposes. '],\n",
       " 126: [' The image displays a line graph with the title \"Histogram of Daily Trading Related Revenue\". The x-axis represents time, starting from December 31 and extending to December 31, 2023. There are two vertical bars on the graph, with one bar reaching almost to the top, indicating a significant value compared to the other.\\n\\nBelow the graph, there is a table with two columns. The left column lists trading-related revenue in millions of dollars, while the right column shows the number of days each value represents. The values range from 1 million to 300 million dollars, and the corresponding numbers of days range from 27 to 1 day.\\n\\nAt the bottom of the image, there\\'s a footnote that provides context for the data: \"YTD (Year To Date) December 31, 2023 compared with December 31, 2022\". The source of the data is also provided at the very bottom, stating \"Yearly Dec. 31, 2022 - Yearly Dec. 31, 2023\".\\n\\nThe style of the image is a standard statistical report, with a focus on presenting quantitative data in an organized and accessible manner. The background color of the graph is light beige, while the bars are a darker shade of gray, which provides contrast for readability. ']}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_description_bofa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the description of the images to be used for the LLM for RAG\n",
    "raw_images = extract_raw_images(pdf_documents[0])\n",
    "text_detected_images = detect_text_in_image(raw_images)\n",
    "img_description_wf = extract_image_description(text_detected_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's read one page using \n",
    "doc = fitz.open(pdf_documents[0])  # Open the PDF file\n",
    "page = doc.load_page(18)  # Load the first page\n",
    "text = page.get_text()  # Extract the text from the page\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxResize(object):\n",
    "    def __init__(self, max_size=800):\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def __call__(self, image):\n",
    "        width, height = image.size\n",
    "        current_max_size = max(width, height)\n",
    "        scale = self.max_size / current_max_size\n",
    "        resized_image = image.resize(\n",
    "            (int(round(scale * width)), int(round(scale * height)))\n",
    "        )\n",
    "\n",
    "        return resized_image\n",
    "\n",
    "\n",
    "detection_transform = transforms.Compose(\n",
    "    [\n",
    "        MaxResize(800),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "structure_transform = transforms.Compose(\n",
    "    [\n",
    "        MaxResize(1000),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# load table detection model\n",
    "# processor = TableTransformerImageProcessor(max_size=800)\n",
    "model = AutoModelForObjectDetection.from_pretrained(\n",
    "    \"microsoft/table-transformer-detection\", revision=\"no_timm\"\n",
    ").to(device)\n",
    "\n",
    "# load table structure recognition model\n",
    "# structure_processor = TableTransformerImageProcessor(max_size=1000)\n",
    "structure_model = AutoModelForObjectDetection.from_pretrained(\n",
    "    \"microsoft/table-transformer-structure-recognition-v1.1-all\"\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# for output bounding box post-processing\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(-1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h), (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=1)\n",
    "\n",
    "\n",
    "def rescale_bboxes(out_bbox, size):\n",
    "    width, height = size\n",
    "    boxes = box_cxcywh_to_xyxy(out_bbox)\n",
    "    boxes = boxes * torch.tensor(\n",
    "        [width, height, width, height], dtype=torch.float32\n",
    "    )\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def outputs_to_objects(outputs, img_size, id2label):\n",
    "    m = outputs.logits.softmax(-1).max(-1)\n",
    "    pred_labels = list(m.indices.detach().cpu().numpy())[0]\n",
    "    pred_scores = list(m.values.detach().cpu().numpy())[0]\n",
    "    pred_bboxes = outputs[\"pred_boxes\"].detach().cpu()[0]\n",
    "    pred_bboxes = [\n",
    "        elem.tolist() for elem in rescale_bboxes(pred_bboxes, img_size)\n",
    "    ]\n",
    "\n",
    "    objects = []\n",
    "    for label, score, bbox in zip(pred_labels, pred_scores, pred_bboxes):\n",
    "        class_label = id2label[int(label)]\n",
    "        if not class_label == \"no object\":\n",
    "            objects.append(\n",
    "                {\n",
    "                    \"label\": class_label,\n",
    "                    \"score\": float(score),\n",
    "                    \"bbox\": [float(elem) for elem in bbox],\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return objects\n",
    "\n",
    "\n",
    "def detect_and_crop_save_table(\n",
    "    file_path, cropped_table_directory=\"./table_images/\"\n",
    "):\n",
    "    image = Image.open(file_path)\n",
    "\n",
    "    filename, _ = os.path.splitext(file_path.split(\"/\")[-1])\n",
    "\n",
    "    if not os.path.exists(cropped_table_directory):\n",
    "        os.makedirs(cropped_table_directory)\n",
    "\n",
    "    # prepare image for the model\n",
    "    # pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "    pixel_values = detection_transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values)\n",
    "\n",
    "    # postprocess to get detected tables\n",
    "    id2label = model.config.id2label\n",
    "    id2label[len(model.config.id2label)] = \"no object\"\n",
    "    detected_tables = outputs_to_objects(outputs, image.size, id2label)\n",
    "\n",
    "    print(f\"number of tables detected {len(detected_tables)}\")\n",
    "\n",
    "    for idx in range(len(detected_tables)):\n",
    "        #   # crop detected table out of image\n",
    "        cropped_table = image.crop(detected_tables[idx][\"bbox\"])\n",
    "        cropped_table.save(f\"./{cropped_table_directory}/{filename}_{idx}.png\")\n",
    "\n",
    "\n",
    "def plot_images(image_paths):\n",
    "    images_shown = 0\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    for img_path in image_paths:\n",
    "        if os.path.isfile(img_path):\n",
    "            image = Image.open(img_path)\n",
    "\n",
    "            plt.subplot(2, 3, images_shown + 1)\n",
    "            plt.imshow(image)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "\n",
    "            images_shown += 1\n",
    "            if images_shown >= 9:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Indexing : Load & Split\n",
    "\n",
    "We need to first load the contents from the PDF file. We will use the [DocumentLoaders](https://python.langchain.com/docs/modules/data_connection/document_loaders/), which are objects that load in data from a source and return a list of Documents. A Document is an object with some page_content (str) and metadata (dict). <br>\n",
    "The UnstructuredPDFLoader is used in the usecase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_one_characters(text):\n",
    "    if re.match('^.$', text):\n",
    "        return ''\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def remove_orientation_text(text):\n",
    "    # There are texts in the pdf that is written vertically which makes the text to appear as\n",
    "    # character space character & so on. This function removes those texts.\n",
    "    if re.match('^(.\\s)+.$', text):\n",
    "        return ''\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def remove_page_number_details(text):\n",
    "    pattern1 = r'\\b\\d+\\s*\\|?\\s*bank of america'\n",
    "    if re.search(pattern1, text, re.IGNORECASE):\n",
    "        return ''\n",
    "    \n",
    "    pattern2 = r'bank of america\\s*2022\\s*\\|?\\s*\\d+'\n",
    "    if re.search(pattern2, text, re.IGNORECASE):\n",
    "        return ''\n",
    "    \n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# While using hi_res_model_name=\"detectron2_onnx\" it will give error - \"Unable to get page count. Is poppler installed and in PATH?\"\n",
    "# perform steps mentioned in [pdf2image](https://github.com/Belval/pdf2image?tab=readme-ov-file#windows) to install poppler\n",
    "loaders = [UnstructuredPDFLoader(os.path.join(os.getcwd(),file), mode = 'elements', \\\n",
    "                                 strategy = 'hi_res', hi_res_model_name=\"detectron2_onnx\", \\\n",
    "                                post_processors = [remove_one_characters, remove_orientation_text,\n",
    "                                                    remove_page_number_details, \\\n",
    "                                                   clean_extra_whitespace]) for file in pdf_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<langchain_community.document_loaders.pdf.UnstructuredPDFLoader at 0x1f28476e140>,\n",
       " <langchain_community.document_loaders.pdf.UnstructuredPDFLoader at 0x1f28476dba0>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of data loaders created is proportional to the number of documents\n",
    "loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asked to install pdf2image, pdfminer.six (do not install pdfminer as it is not actively maintained instead\n",
    "# use pdfminer.six), opencv-python, unstructured_inference & many more\n",
    "# Instead of installing individual dependent libraries for UnstructuredPDF, install unstructured using\n",
    "# pip install unstructured[pdf] \n",
    "doc_0 = loaders[0].load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid printing multiple text on GitHub\n",
    "for doc in doc_0:\n",
    "    if False:\n",
    "        print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_1 = loaders[1].load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid printing multiple text on GitHub\n",
    "for doc1 in doc_1:\n",
    "    if False:\n",
    "        print(doc1.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the page numbers along with text \"Wells Fargo & Company\" in the footer.\n",
    "previous_text = ''\n",
    "for i, doc1 in enumerate(doc_1):\n",
    "    if re.match('^\\d+$', doc1.page_content):\n",
    "        previous_text = doc1.page_content\n",
    "        continue\n",
    "    \n",
    "    if re.match('^\\d+$', previous_text) and re.search('^Wells Fargo & Company$', doc1.page_content, re.IGNORECASE):  \n",
    "        doc1.page_content = ''           \n",
    "    else:\n",
    "        previous_text = ''   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_text = ''\n",
    "for i, doc1 in enumerate(doc_1):\n",
    "    if re.search('^Wells Fargo & Company$', doc1.page_content, re.IGNORECASE):\n",
    "        previous_text = doc1.page_content\n",
    "        continue\n",
    "    \n",
    "    if re.search('^Wells Fargo & Company$', previous_text, re.IGNORECASE) and re.match('^\\d+$', doc1.page_content):        \n",
    "        doc1.page_content = '' \n",
    "    else:\n",
    "        previous_text = ''    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = list(filter(lambda x: x.page_content != '', doc_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc1 in doc_1:\n",
    "    # To avoid printing multiple text on GitHub\n",
    "    if False:\n",
    "        print(doc1.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n"
     ]
    }
   ],
   "source": [
    "max_length = max(len(content_bofa.page_content) for content_bofa in doc_0)\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3935\n"
     ]
    }
   ],
   "source": [
    "max_length = max(len(content_wf.page_content) for content_wf in doc_1)\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Indexing : Store\n",
    "Creating embeddings for the splitted data and store the documents and it's corresponsing embeddings in a vector store. At this point we have a query-able vector store containing the chunked contents of our PDF's. Given a user question, we should ideally be able to return the snippets of the text that answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(documents=doc_0+doc_1, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.save_local('vectorstore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\baira\\Desktop\\Infy_Tech_Pioneer\\InfyTech_Docs_ChatBot\\.venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "vectorstore = FAISS.load_local(\"./vectorstore\",OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Brian T. Moynihan Chair of the Board and Chief Executive Oﬃcer, Bank of America Corporation', metadata={'source': 'c:\\\\Users\\\\baira\\\\Desktop\\\\Infy_Tech_Pioneer\\\\InfyTech_Docs_ChatBot\\\\2022-annual-report-bofa.pdf', 'detection_class_prob': 0.8070501685142517, 'coordinates': {'points': ((107.71715497970581, 278.45842002094656), (107.71715497970581, 401.0500022409043), (392.23714805555574, 401.0500022409043), (392.23714805555574, 278.45842002094656)), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2024-01-04T14:51:31', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 223, 'parent_id': 'dc1fb50caf875b9f261d70b0be8ba6e3', 'file_directory': 'c:\\\\Users\\\\baira\\\\Desktop\\\\Infy_Tech_Pioneer\\\\InfyTech_Docs_ChatBot', 'filename': '2022-annual-report-bofa.pdf', 'category': 'NarrativeText'}), Document(page_content='Bank of America Corporation', metadata={'source': 'c:\\\\Users\\\\baira\\\\Desktop\\\\Infy_Tech_Pioneer\\\\InfyTech_Docs_ChatBot\\\\2022-annual-report-bofa.pdf', 'coordinates': {'points': ((170.95916666666665, 1802.3374652777777), (170.95916666666665, 1846.61125), (325.07243055555557, 1846.61125), (325.07243055555557, 1802.3374652777777)), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2024-01-04T14:51:31', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 60, 'file_directory': 'c:\\\\Users\\\\baira\\\\Desktop\\\\Infy_Tech_Pioneer\\\\InfyTech_Docs_ChatBot', 'filename': '2022-annual-report-bofa.pdf', 'category': 'Title'}), Document(page_content='Bank of America Corporation', metadata={'source': 'c:\\\\Users\\\\baira\\\\Desktop\\\\Infy_Tech_Pioneer\\\\InfyTech_Docs_ChatBot\\\\2022-annual-report-bofa.pdf', 'coordinates': {'points': ((132.30527777777783, 963.0863888888888), (132.30527777777783, 982.5308333333332), (375.2091666666668, 982.5308333333332), (375.2091666666668, 963.0863888888888)), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2024-01-04T14:51:31', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 107, 'file_directory': 'c:\\\\Users\\\\baira\\\\Desktop\\\\Infy_Tech_Pioneer\\\\InfyTech_Docs_ChatBot', 'filename': '2022-annual-report-bofa.pdf', 'category': 'Title'})]\n"
     ]
    }
   ],
   "source": [
    "# Similarity Search\n",
    "query = \"Bank of America Chair & CEO?\"\n",
    "docs = vectorstore.similarity_search(query, k=3)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Chief Operations Executive', metadata={'source': 'c:\\\\Users\\\\baira\\\\Desktop\\\\Infy_Tech_Pioneer\\\\InfyTech_Docs_ChatBot\\\\2022-annual-report-bofa.pdf', 'coordinates': {'points': ((605.015, 1808.9855555555553), (605.015, 1859.1568055555556), (780.1622222222222, 1859.1568055555556), (780.1622222222222, 1808.9855555555553)), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2024-01-04T14:51:31', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 15, 'file_directory': 'c:\\\\Users\\\\baira\\\\Desktop\\\\Infy_Tech_Pioneer\\\\InfyTech_Docs_ChatBot', 'filename': '2022-annual-report-bofa.pdf', 'category': 'Title'}), Document(page_content='Chair of the Board and Chief Executive Oﬃcer', metadata={'source': 'c:\\\\Users\\\\baira\\\\Desktop\\\\Infy_Tech_Pioneer\\\\InfyTech_Docs_ChatBot\\\\2022-annual-report-bofa.pdf', 'coordinates': {'points': ((134.71444444444444, 444.44972222222214), (134.71444444444444, 500.5497222222222), (362.2334722222222, 500.5497222222222), (362.2334722222222, 444.44972222222214)), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2024-01-04T14:51:31', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 14, 'file_directory': 'c:\\\\Users\\\\baira\\\\Desktop\\\\Infy_Tech_Pioneer\\\\InfyTech_Docs_ChatBot', 'filename': '2022-annual-report-bofa.pdf', 'category': 'Title'}), Document(page_content='Chair of the Board and Chief Executive Oﬃcer', metadata={'source': 'c:\\\\Users\\\\baira\\\\Desktop\\\\Infy_Tech_Pioneer\\\\InfyTech_Docs_ChatBot\\\\2022-annual-report-bofa.pdf', 'coordinates': {'points': ((107.91833333333334, 442.4883333333333), (107.91833333333334, 495.6133333333333), (346.1780555555555, 495.6133333333333), (346.1780555555555, 442.4883333333333)), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2024-01-04T14:51:31', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 15, 'file_directory': 'c:\\\\Users\\\\baira\\\\Desktop\\\\Infy_Tech_Pioneer\\\\InfyTech_Docs_ChatBot', 'filename': '2022-annual-report-bofa.pdf', 'category': 'Title'})]\n"
     ]
    }
   ],
   "source": [
    "# Similarity Search\n",
    "query = \"Who is the CEO of Amazon?\"\n",
    "docs = vectorstore.similarity_search(query, k=3)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Retrieval and Generation: Retrieve\n",
    "Different ways to retrive documents based on query - [Link](https://python.langchain.com/docs/modules/data_connection/retrievers/vectorstore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Using the query as is to retrieve the relevant documents using the search type \"similarity_score_threshold\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_similarity = vectorstore.as_retriever(search_type=\"similarity_score_threshold\",\n",
    "                                                 search_kwargs={\"score_threshold\": 0.70})\n",
    "# retrieved_docs = retriever.invoke(\"How innovation is driven at bank of america?\")\n",
    "retrieved_docs = retriever_similarity.get_relevant_documents(\"Who is the CEO of Amazon\") #What is the full-form of ROTCE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chief Operations Executive\n",
      "Chair of the Board and Chief Executive Oﬃcer\n",
      "Chair of the Board and Chief Executive Oﬃcer\n",
      "CEO Letter\n"
     ]
    }
   ],
   "source": [
    "for doc in retrieved_docs:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Using the Multi Query retriver which will create variants of queries based on the prompt to retrieve the relevant documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\baira\\Desktop\\Infy_Tech_Pioneer\\InfyTech_Docs_ChatBot\\.venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# supply a prompt along with an output parser to split the results into a list of queries.\n",
    "# Output parser will split the LLM result into a list of queries\n",
    "class LineList(BaseModel):\n",
    "    # \"lines\" is the key (attribute name) of the parsed output\n",
    "    lines: List[str] = Field(description=\"Lines of text\")\n",
    "\n",
    "\n",
    "class LineListOutputParser(PydanticOutputParser):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(pydantic_object=LineList)\n",
    "\n",
    "    def parse(self, text: str) -> LineList:\n",
    "        lines = text.strip().split(\"\\n\")\n",
    "        return LineList(lines=lines)\n",
    "\n",
    "\n",
    "output_parser = LineListOutputParser()\n",
    "\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate four \\ \n",
    "    different versions of the given user question to retrieve relevant documents from a vector \\\n",
    "    database. By generating multiple perspectives on the user question, your goal is to help \\\n",
    "    the user overcome some of the limitations of the distance-based similarity search. \\ \n",
    "    Provide these alternative questions separated by newlines. \\\n",
    "    Original question: {question}\"\"\",\n",
    ")\n",
    "llm = ChatOpenAI(temperature=0, model='gpt-3.5-turbo-16k')\n",
    "\n",
    "\n",
    "# Chain\n",
    "llm_chain = LLMChain(llm=llm, prompt=QUERY_PROMPT, output_parser=output_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run\n",
    "retriever = MultiQueryRetriever(\n",
    "    include_original=True,\n",
    "    retriever=vectorstore.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.70}),\n",
    "    llm_chain=llm_chain, parser_key=\"lines\",\n",
    "    verbose=True\n",
    ")  # \"lines\" is the key (attribute name) of the parsed output\n",
    "\n",
    "# Results\n",
    "unique_docs = retriever.get_relevant_documents(\n",
    "    query=\"What is tuition assistance?\"\n",
    ")\n",
    "len(unique_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is tuition assistance?',\n",
       " 'text': LineList(lines=['1. Can you explain the concept of tuition assistance?', '2. How does tuition assistance work?', '3. What are the benefits of tuition assistance programs?', '4. Can you provide an overview of tuition assistance options available?'])}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To check different queries generated by the llm\n",
    "llm_chain.invoke(\"What is tuition assistance?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Retrieval and Generation: Generate\n",
    "In this step everything will be put together into a chain. A chain will take question, will retrieve relevant documents, will construct a prompt, will pass to the llm model and will parse the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contextualize_ques_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "# which might refer to a context in the chat history, formulate a standalone question \\\n",
    "# which can be understood without the chat history. Do NOT answer the question, \\\n",
    "# just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "\n",
    "contextualize_ques_system_prompt = \"\"\" Check if the latest user question refers to a context in the chat history. \\\n",
    "If it does, formulate a standalone question which can be understood without the chat history. \\\n",
    "If it does not, return the question as is. Your task is to just reformulate the question if needed.\n",
    "\"\"\"\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_ques_system_prompt),\n",
    "        # Prompt template that assumes variable is already list of messages.\n",
    "        # We provide the variable name to be used as messages\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "contextualize_q_chain = contextualize_q_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just say that you don't know.\n",
    "# qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "# Use the following pieces of retrieved context to answer the question. \\\n",
    "# If you don't know the answer, provide a response as - \"I don't know.\" \\\n",
    "\n",
    "# {context}\"\"\"\n",
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following context or document embeddings to answer the question. \\\n",
    "Don't justify your answers and don't search the internet.If the answer could not be found in the documents,\\\n",
    "ay the words \"Sorry, I am unable to answer your question with the information available to me\"\\\n",
    "\n",
    "{context}\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def contextualized_question(input: dict):\n",
    "    if input.get(\"chat_history\"):        \n",
    "        return contextualize_q_chain\n",
    "    else:\n",
    "        return input[\"question\"]\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        context=contextualized_question | retriever_similarity\n",
    "    )\n",
    "    | qa_prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  context: RunnableLambda(contextualized_question)\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001620040C040>, search_type='similarity_score_threshold', search_kwargs={'score_threshold': 0.7})\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['chat_history', 'context', 'question'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template='You are an assistant for question-answering tasks. Use the following context or document embeddings to answer the question. Don\\'t justify your answers and don\\'t search the internet.If the answer could not be found in the documents,ay the words \"Sorry, I am unable to answer your question with the information available to me\"\\n{context}')), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001625010F250>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001620040CAF0>, model_name='gpt-3.5-turbo-16k', temperature=0.0, openai_api_key='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', openai_proxy='')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "\n",
    "question = \"How Risk Management is done in Wells Fargo?\"\n",
    "ai_msg = rag_chain.invoke({\"question\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=question), ai_msg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Wells Fargo manages a variety of risks that can significantly affect its financial performance and its ability to meet the expectations of its customers, shareholders, regulators, and other stakeholders. The company's top priority is to strengthen its risk and control infrastructure. Wells Fargo continues to enhance and mature its risk management programs, including operational and compliance risk management programs as required by regulatory orders. The company is also actively involved in industry cybersecurity efforts and works with third-party service providers and governmental agencies to enhance defenses and improve resiliency to information security threats.\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_msg.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_question = \"How Wells Fargo protect against unauthorized access?\"\n",
    "new_msg = rag_chain.invoke({\"question\": second_question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=second_question), new_msg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wells Fargo prioritizes the protection of its networks, computers, software, and data from unauthorized access. The company implements controls, processes, and systems designed to enhance information security and prevent unauthorized access. These measures include robust authentication protocols, encryption technologies, firewalls, intrusion detection systems, and regular monitoring of network activity. Wells Fargo also collaborates with third-party service providers and governmental agencies to enhance defenses and improve resiliency to information security threats.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_msg.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_question = \"Who is the CEO of Amazon?\"\n",
    "new_msg = rag_chain.invoke({\"question\": third_question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=third_question), new_msg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sorry, I am unable to answer your question with the information available to me.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_msg.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourth_question = \"What did Sheri Bronstein wrote in her letter in Bank of America document?\"\n",
    "new_msg = rag_chain.invoke({\"question\": fourth_question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=fourth_question), new_msg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I apologize for the confusion, but I don't have access to the specific content of Sheri Bronstein's letter in the Bank of America document.\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_msg.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Emotional wellness refers to the state of one's emotional well-being and the ability to effectively cope with and manage emotions. It involves understanding and expressing emotions in a healthy way, having positive relationships, and maintaining a sense of balance and resilience in the face of challenges. Emotional wellness encompasses self-awareness, self-care, stress management, and the ability to navigate and regulate emotions in a constructive manner. It is an important aspect of overall well-being and contributes to a person's mental and psychological health.\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fifth_question = \"What is Emotional Wellness?\"\n",
    "new_msg = rag_chain.invoke({\"question\": fifth_question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=fifth_question), new_msg])\n",
    "new_msg.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m third_question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are different types of Risk Management at Wells Fargo?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mrag_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mthird_question\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchat_history\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mchat_history\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[1;32mc:\\Users\\baira\\Desktop\\Infy_Tech_Pioneer\\InfyTech_Docs_ChatBot\\.venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:2053\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m   2051\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2052\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[1;32m-> 2053\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2054\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2055\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[0;32m   2056\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2057\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2058\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2059\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2060\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   2061\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\baira\\Desktop\\Infy_Tech_Pioneer\\InfyTech_Docs_ChatBot\\.venv\\lib\\site-packages\\langchain_core\\runnables\\passthrough.py:415\u001b[0m, in \u001b[0;36mRunnableAssign.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;28minput\u001b[39m: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m    412\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    413\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    414\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m--> 415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\baira\\Desktop\\Infy_Tech_Pioneer\\InfyTech_Docs_ChatBot\\.venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:1246\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m   1242\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[0;32m   1243\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(var_child_runnable_config\u001b[38;5;241m.\u001b[39mset, child_config)\n\u001b[0;32m   1244\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[0;32m   1245\u001b[0m         Output,\n\u001b[1;32m-> 1246\u001b[0m         context\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m   1247\u001b[0m             call_func_with_variable_args,\n\u001b[0;32m   1248\u001b[0m             func,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1249\u001b[0m             \u001b[38;5;28minput\u001b[39m,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1250\u001b[0m             config,\n\u001b[0;32m   1251\u001b[0m             run_manager,\n\u001b[0;32m   1252\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1253\u001b[0m         ),\n\u001b[0;32m   1254\u001b[0m     )\n\u001b[0;32m   1255\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1256\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mc:\\Users\\baira\\Desktop\\Infy_Tech_Pioneer\\InfyTech_Docs_ChatBot\\.venv\\lib\\site-packages\\langchain_core\\runnables\\config.py:326\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    325\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[1;32m--> 326\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\baira\\Desktop\\Infy_Tech_Pioneer\\InfyTech_Docs_ChatBot\\.venv\\lib\\site-packages\\langchain_core\\runnables\\passthrough.py:402\u001b[0m, in \u001b[0;36mRunnableAssign._invoke\u001b[1;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_invoke\u001b[39m(\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;28minput\u001b[39m: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    395\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m    396\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    397\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mdict\u001b[39m\n\u001b[0;32m    398\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe input to RunnablePassthrough.assign() must be a dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    401\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m--> 402\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmapper\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m    403\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    404\u001b[0m             patch_config(config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child()),\n\u001b[0;32m    405\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    406\u001b[0m         ),\n\u001b[0;32m    407\u001b[0m     }\n",
      "File \u001b[1;32mc:\\Users\\baira\\Desktop\\Infy_Tech_Pioneer\\InfyTech_Docs_ChatBot\\.venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:2692\u001b[0m, in \u001b[0;36mRunnableParallel.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m   2679\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m   2680\u001b[0m         futures \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   2681\u001b[0m             executor\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[0;32m   2682\u001b[0m                 step\u001b[38;5;241m.\u001b[39minvoke,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2690\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   2691\u001b[0m         ]\n\u001b[1;32m-> 2692\u001b[0m         output \u001b[38;5;241m=\u001b[39m {key: future\u001b[38;5;241m.\u001b[39mresult() \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[0;32m   2693\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   2694\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\baira\\Desktop\\Infy_Tech_Pioneer\\InfyTech_Docs_ChatBot\\.venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:2692\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2679\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m   2680\u001b[0m         futures \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   2681\u001b[0m             executor\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[0;32m   2682\u001b[0m                 step\u001b[38;5;241m.\u001b[39minvoke,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2690\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   2691\u001b[0m         ]\n\u001b[1;32m-> 2692\u001b[0m         output \u001b[38;5;241m=\u001b[39m {key: \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[0;32m   2693\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   2694\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py:440\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m--> 440\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "third_question = \"What are different types of Risk Management at Wells Fargo?\"\n",
    "rag_chain.invoke({\"question\": third_question, \"chat_history\": chat_history}).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear chat history\n",
    "chat_history.clear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
